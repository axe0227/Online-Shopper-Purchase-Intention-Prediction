---
title: "R Notebook"
output: html_notebook
---
```{r}

```

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.


```{r}
library(data.table)
library(mltools)
library(dplyr)
```


```{r}
setwd("~/Desktop/stat/final")
os <- read.csv(file = "online_shoppers_intention.csv")
num_duplicated <- nrow(os[duplicated(os),])
os <- os[!duplicated(os),] # remove dumplicated rows
```

#filtering missing values
```{r}
which(is.na(os))
```
# no missing values

```{r}
sapply(os,class)
```

```{r}
barplot(table(os$Revenue),ylim = c(0,12000), main = "Target Feature Distribution", xlab = "Target Feature", col = 'steelblue')
```


```{r}
os
```

```{r}
os <- os %>% 
  mutate(Month = as.factor(Month),
         OperatingSystems = as.factor(OperatingSystems),
         Browser = as.factor(Browser),
         Region = as.factor(Region),
         TrafficType = as.factor(TrafficType),
         VisitorType = as.factor(VisitorType),
         Weekend = as.integer(Weekend),
         Revenue = as.integer(Revenue)
         )
```


#1-of-C encodings
```{r}
os <- one_hot(as.data.table(os))
```

```{r}
os
```

```{r}
#set.seed(10)
#train=sample(1:nrow(os),ceil(0.7*nrow(os)))
#train_data = os[train,]
#test=-train
#test_data = os[test,]
```


```{r}
#train_numerical <- train_data[,1:10] 
#train_categorical <- train_data[,11:75]
#test_numerical <- test_data[,1:10] 
#test_categorical = test_data[,11:75]
#train_scaled = scale(train_numerical)
#test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
#train_data1 <- cbind(train_scaled, train_categorical)
#test_data1 <- cbind(test_scaled, test_categorical)
```

```{r}
library(randomForest)
library(pracma)
set.seed(1)
train=sample(1:nrow(os),ceil(0.7*nrow(os)))
train_data = os[train,]
test=-train
test_data = os[test,]
#train_numerical <- train_data[,1:10] 
#train_categorical <- train_data[,11:17]
#test_numerical <- test_data[,1:10] 
#test_categorical = test_data[,11:75]
#train_scaled = scale(train_numerical)
#test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
#train_data1 <- cbind(train_scaled, train_categorical)
#est_data1 <- cbind(test_scaled, test_categorical)

N_1 = 2*length( which(train_data$Revenue == 0))
os_over <- ovun.sample(Revenue~.,data = train_data, method= 'over', N = N_1, seed = 1)$data
n<-length(names(os_over)) 
m = ceil(log2(n))
rf_train<-randomForest(as.factor(os_over$Revenue)~.,data=os_over,mtry=m ,ntree=100,importance=TRUE,proximity=TRUE)
#predict
pred<-predict(rf_train,newdata=test_data)
```

```{r}
varImpPlot(rf_train, sort = TRUE, n.var = 17, main = 'Features Importance by RF', type = 1)
```
```{r}
os_optimal = subset(os, select = c("PageValues","Month","ProductRelated","ExitRates","ProductRelated_Duration","Administrative_Duration","BounceRates","Region","Administrative","TrafficType","Browser","OperatingSystems","Informational_Duration" ,"Revenue"))
```


```{r}
set.seed(1)
train=sample(1:nrow(os_optimal),ceil(0.7*nrow(os)))
train_data = os_optimal[train,]
test=-train
test_data = os_optimal[test,]
N_1 = 2*length( which(train_data$Revenue == 0))
os_over <- ovun.sample(Revenue~.,data = train_data, method= 'over', N = N_1, seed = 1)$data
n<-length(names(os_over)) 
m = ceil(log2(n))
rf_train<-randomForest(as.factor(os_over$Revenue)~.,data=os_over,mtry=m ,ntree=100,importance=TRUE,proximity=TRUE)
#predict
pred<-predict(rf_train,newdata=test_data)
```

```{r}
tab <- table(pred, test_data$Revenue)
sum_tab = colSums(tab)[1] + colSums(tab)[2] 
Accuracy = sum(diag(tab))/sum_tab
TPR= table(pred, test_data$Revenue)[1]/colSums(table(pred, test_data$Revenue))[1]
TNR = table(pred, test_data$Revenue)[4]/colSums(table(pred, test_data$Revenue))[2]
PPV = table(pred, test_data$Revenue)[1]/rowSums(table(pred, test_data$Revenue))[1]
F1_neg = 2*PPV*TPR/(PPV+TPR)

TPR = table(pred, test_data$Revenue)[4]/colSums(table(pred, test_data$Revenue))[2]
TNR = table(pred, test_data$Revenue)[1]/colSums(table(pred, test_data$Revenue))[1]
PPV = table(pred, test_data$Revenue)[4]/rowSums(table(pred, test_data$Revenue))[2]
F1_pos = 2*PPV*TPR/(PPV+TPR)

F1 = (F1_pos + F1_neg)/2
Accuracy_rf_rfe = Accuracy
TPR_rf_rfe = TPR
TNR_rf_rfe = TNR
F1_rf_rfe = F1
```
```{r}
Accuracy_rf_rfe 
TPR_rf_rfe 
TNR_rf_rfe 
F1_rf_rfe
```

```{r}
library(mlbench)
library(caret)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(as.factor(os_over$Revenue)~., data=os_over, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
```

```{r}
library(mlbench)
library(caret)
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(os_over[,1:17], as.factor(os_over[,18]), sizes=c(1:17), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```

```{r}
str(results)
```

```{r}
#Random Forest
os1 <- os %>% 
  mutate(Month = as.integer(Month),
         OperatingSystems = as.integer(OperatingSystems),
         Browser = as.integer(Browser),
         Region = as.integer(Region),
         TrafficType = as.integer(TrafficType),
         VisitorType = as.integer(VisitorType),
         Weekend = as.integer(Weekend),
         Revenue = as.integer(Revenue)
         )

library(ROSE)
library(randomForest)
Accuracy = rep(0, 10)
TPR = rep(0, 10)
TNR = rep(0, 10)
PPV = rep(0, 10)
F1_pos = rep(0, 10)
F1_neg = rep(0, 10)
F1 = rep(0, 10)


for (i in 1:10){
  set.seed(i)
  train=sample(1:nrow(os1),ceil(0.7*nrow(os)))
  train_data = os1[train,]
  test=-train
  test_data = os1[test,]
  #train_numerical <- train_data[,1:10] 
  #train_categorical <- train_data[,11:75]
  #test_numerical <- test_data[,1:10] 
  #test_categorical = test_data[,11:75]
  #train_scaled = scale(train_numerical)
  #test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
  #train_data1 <- cbind(train_scaled, train_categorical)
  #test_data1 <- cbind(test_scaled, test_categorical)
  
  N_1 = 2*length( which(train_data$Revenue == 0))
  os_over <- ovun.sample(Revenue~.,data = train_data, method= 'over', N = N_1, seed = 1)$data
  n<-length(names(os_over)) 
  m = ceil(log2(n))
  rf_train<-randomForest(as.factor(os_over$Revenue)~.,data=os_over,mtry=m ,ntree=100,importance=TRUE,proximity=TRUE)
  #predict
  pred<-predict(rf_train,newdata=test_data1)
  Accuracy[i] = sum(diag(table(pred, test_data1$Revenue)))/sum(table(pred, test_data1$Revenue))

  TPR[i] = table(pred, test_data1$Revenue)[1]/colSums(table(pred, test_data1$Revenue))[1]
  TNR[i] = table(pred, test_data1$Revenue)[4]/colSums(table(pred, test_data1$Revenue))[2]
  PPV[i] = table(pred, test_data1$Revenue)[1]/rowSums(table(pred, test_data1$Revenue))[1]
  F1_neg[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  TPR[i] = table(pred, test_data1$Revenue)[4]/colSums(table(pred, test_data1$Revenue))[2]
  TNR[i] = table(pred, test_data1$Revenue)[1]/colSums(table(pred, test_data1$Revenue))[1]
  PPV[i] = table(pred, test_data1$Revenue)[4]/rowSums(table(pred, test_data1$Revenue))[2]
  F1_pos[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  F1[i] = (F1_pos[i] + F1_neg[i])/2
}
Accuracy_rf = mean(Accuracy)
TPR_rf = mean(TPR)
TNR_rf = mean(TNR)
F1_rf = mean(F1)
```

```{r}
#cm = confusionMatrix(pred, as.factor(test_data1$Revenue))
```

```{r}
#cm
```
```{r}
#library(MLmetrics)
#recall = Recall(as.factor(test_data1$Revenue), pred)
```
```{r}
#precision = Precision(as.factor(test_data1$Revenue), pred)
```

```{r}
#f1 = 2 * precision * recall / (precision + recall)
```

```{r}
#F1_Score(as.factor(test_data1$Revenue), pred)
```

```{r}
#str(cm)
```

#c4.5
```{r}
#library(RWeka)
library(party)
Accuracy = rep(0, 100)
TPR = rep(0, 100)
TNR = rep(0, 100)
PPV = rep(0, 100)
F1_pos = rep(0, 100)
F1_neg = rep(0, 100)
F1 = rep(0, 100)
for (i in 1:100){
  set.seed(i)
  train=sample(1:nrow(os),ceil(0.7*nrow(os)))
  train_data = os[train,]
  test=-train
  test_data = os[test,]
  train_numerical <- train_data[,1:10] 
  train_categorical <- train_data[,11:75]
  test_numerical <- test_data[,1:10] 
  test_categorical = test_data[,11:75]
  train_scaled = scale(train_numerical)
  test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
  train_data1 <- cbind(train_scaled, train_categorical)
  test_data1 <- cbind(test_scaled, test_categorical)
  
  N_1 = 2*length( which(train_data1$Revenue == 0))
  os_over <- ovun.sample(Revenue~.,data = train_data1, method= 'over', N = N_1, seed = 1)$data
  my_ctree<-ctree(as.factor(os_under$Revenue)~.,data=os_over)
  #predict
  pred<-predict(my_ctree,newdata=test_data1)
  library(tree)
  library(dtree)
  library(rpart)
  ptree=prune.rpart(my_ctree,cp=my_ctree$Revenue[which.min(my_ctree$Revenue[,"x error"]),"CP"])
  pred<-predict(ptree,newdata=test_data1) 
  Accuracy[i] = sum(diag(table(pred, test_data1$Revenue)))/sum(table(pred, test_data1$Revenue))

  TPR[i] = table(pred, test_data1$Revenue)[1]/colSums(table(pred, test_data1$Revenue))[1]
  TNR[i] = table(pred, test_data1$Revenue)[4]/colSums(table(pred, test_data1$Revenue))[2]
  PPV[i] = table(pred, test_data1$Revenue)[1]/rowSums(table(pred, test_data1$Revenue))[1]
  F1_neg[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  TPR[i] = table(pred, test_data1$Revenue)[4]/colSums(table(pred, test_data1$Revenue))[2]
  TNR[i] = table(pred, test_data1$Revenue)[1]/colSums(table(pred, test_data1$Revenue))[1]
  PPV[i] = table(pred, test_data1$Revenue)[4]/rowSums(table(pred, test_data1$Revenue))[2]
  F1_pos[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  F1[i] = (F1_pos[i] + F1_neg[i])/2
}
Accuracy_rf = mean(Accuracy)
TPR_rf = mean(TPR)
TNR_rf = mean(TNR)
F1_rf = mean(F1)
```

```{r}
#Accuracy_rf 
#TPR_rf 
#TNR_rf 
#F1_rf 
```

#SVM_Linear
```{r}
library("e1071")
library(ROSE)
Accuracy = rep(0, 100)
TPR = rep(0, 100)
TNR = rep(0, 100)
PPV = rep(0, 100)
F1_pos = rep(0, 100)
F1_neg = rep(0, 100)
F1 = rep(0, 100)
for (i in 1:100){
  set.seed(i)
  train=sample(1:nrow(os),ceil(0.7*nrow(os)))
  train_data = os[train,]
  test=-train
  test_data = os[test,]
  train_numerical <- train_data[,1:10] 
  train_categorical <- train_data[,11:75]
  test_numerical <- test_data[,1:10] 
  test_categorical = test_data[,11:75]
  train_scaled = scale(train_numerical)
  test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
  train_data1 <- cbind(train_scaled, train_categorical)
  test_data1 <- cbind(test_scaled, test_categorical)
  
  N_1 = 2*length( which(train_data1$Revenue == 0))
  os_under <- ovun.sample(Revenue~.,data = train_data1, method= 'over', N = N_1, seed = 1)$data
  svmfit = svm(as.factor(Revenue)~., data=os_under, kernel = "linear", cost = 4)
  #predict
  pred <- predict(svmfit, newdata = test_data1)
  Accuracy[i] = sum(diag(table(pred, test_data1$Revenue)))/sum(table(pred, test_data1$Revenue))

  TPR[i] = table(pred, test_data1$Revenue)[1]/colSums(table(pred, test_data1$Revenue))[1]
  TNR[i] = table(pred, test_data1$Revenue)[4]/colSums(table(pred, test_data1$Revenue))[2]
  PPV[i] = table(pred, test_data1$Revenue)[1]/rowSums(table(pred, test_data1$Revenue))[1]
  F1_neg[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  TPR[i] = table(pred, test_data1$Revenue)[4]/colSums(table(pred, test_data1$Revenue))[2]
  TNR[i] = table(pred, test_data1$Revenue)[1]/colSums(table(pred, test_data1$Revenue))[1]
  PPV[i] = table(pred, test_data1$Revenue)[4]/rowSums(table(pred, test_data1$Revenue))[2]
  F1_pos[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  F1[i] = (F1_pos[i] + F1_neg[i])/2
}
Accuracy_rf = mean(Accuracy)
TPR_rf = mean(TPR)
TNR_rf = mean(TNR)
F1_rf = mean(F1)
```

#SVM_RBF
```{r}
library("e1071")
Accuracy = rep(0, 100)
TPR = rep(0, 100)
TNR = rep(0, 100)
PPV = rep(0, 100)
F1_pos = rep(0, 100)
F1_neg = rep(0, 100)
F1 = rep(0, 100)
for (i in 1:100){
  set.seed(i)
  train=sample(1:nrow(os),ceil(0.7*nrow(os)))
  train_data = os[train,]
  test=-train
  test_data = os[test,]
  train_numerical <- train_data[,1:10] 
  train_categorical <- train_data[,11:75]
  test_numerical <- test_data[,1:10] 
  test_categorical = test_data[,11:75]
  train_scaled = scale(train_numerical)
  test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
  train_data1 <- cbind(train_scaled, train_categorical)
  test_data1 <- cbind(test_scaled, test_categorical)
  
  N_1 = 2*length( which(train_data1$Revenue == 0))
  os_under <- ovun.sample(Revenue~.,data = train_data1, method= 'over', N = N_1, seed = 1)$data
  svmfit = svm(as.factor(Revenue)~., data=os_under, kernel = "radial", cost=3.5, gamma=0.013)
  #predict
  pred <- predict(svmfit, newdata = test_data1)
  Accuracy[i] = sum(diag(table(pred, test_data1$Revenue)))/sum(table(pred, test_data1$Revenue))

  TPR[i] = table(pred, test_data1$Revenue)[1]/colSums(table(pred, test_data1$Revenue))[1]
  TNR[i] = table(pred, test_data1$Revenue)[4]/colSums(table(pred, test_data1$Revenue))[2]
  PPV[i] = table(pred, test_data1$Revenue)[1]/rowSums(table(pred, test_data1$Revenue))[1]
  F1_neg[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  TPR[i] = table(pred, test_data1$Revenue)[4]/colSums(table(pred, test_data1$Revenue))[2]
  TNR[i] = table(pred, test_data1$Revenue)[1]/colSums(table(pred, test_data1$Revenue))[1]
  PPV[i] = table(pred, test_data1$Revenue)[4]/rowSums(table(pred, test_data1$Revenue))[2]
  F1_pos[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  F1[i] = (F1_pos[i] + F1_neg[i])/2
}
Accuracy_rf = mean(Accuracy)
TPR_rf = mean(TPR)
TNR_rf = mean(TNR)
F1_rf = mean(F1)
```

#k-Nearest Neighbour, poor performance
```{r}
library(class)
Accuracy = rep(0, 100)
TPR = rep(0, 100)
TNR = rep(0, 100)
PPV = rep(0, 100)
F1_pos = rep(0, 100)
F1_neg = rep(0, 100)
F1 = rep(0, 100)
for (i in 1:100){
  set.seed(i)
  train=sample(1:nrow(os),ceil(0.7*nrow(os)))
  train_data = os[train,]
  test=-train
  test_data = os[test,]
  train_numerical <- train_data[,1:10] 
  train_categorical <- train_data[,11:75]
  test_numerical <- test_data[,1:10] 
  test_categorical = test_data[,11:75]
  train_scaled = scale(train_numerical)
  test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
  train_data1 <- cbind(train_scaled, train_categorical)
  test_data1 <- cbind(test_scaled, test_categorical)
  
  N_1 = 2*length( which(train_data1$Revenue == 0))
  os_under <- ovun.sample(Revenue~.,data = train_data1, method= 'over', N = N_1, seed = 1)$data
  k_nn <- knn(os_under[, 1:29], test_data1[, 1:29], os_under$Revenue, k=110)
  tab <- table(k_nn, test_data1$Revenue)
  #predict
  Accuracy[i] = sum(diag(tab))/sum(tab)

  TPR[i] = tab[1]/colSums(tab)[1]
  TNR[i] = tab[4]/colSums(tab)[2]
  PPV[i] = tab[1]/rowSums(tab)[1]
  F1_neg[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  TPR[i] = tab[4]/colSums(tab)[2]
  TNR[i] = tab[1]/colSums(tab)[1]
  PPV[i] = tab[4]/rowSums(tab)[2]
  F1_pos[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  F1[i] = (F1_pos[i] + F1_neg[i])/2
}
Accuracy_rf = mean(Accuracy)
TPR_rf = mean(TPR)
TNR_rf = mean(TNR)
F1_rf = mean(F1)
```

#Generalized Boosted Regression Models
```{r}
library(mlbench)
library(gbm)
Accuracy = rep(0, 2)
TPR = rep(0, 2)
TNR = rep(0, 2)
PPV = rep(0, 2)
F1_pos = rep(0, 2)
F1_neg = rep(0, 2)
F1 = rep(0, 2)
for (i in 1:2){
  set.seed(i)
  train=sample(1:nrow(os),ceil(0.7*nrow(os)))
  train_data = os[train,]
  test=-train
  test_data = os[test,]
  train_numerical <- train_data[,1:10] 
  train_categorical <- train_data[,11:75]
  test_numerical <- test_data[,1:10] 
  test_categorical = test_data[,11:75]
  train_scaled = scale(train_numerical)
  test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
  train_data1 <- cbind(train_scaled, train_categorical)
  test_data1 <- cbind(test_scaled, test_categorical)
  
  N_1 = 2*length( which(train_data1$Revenue == 0))
  os_over <- ovun.sample(Revenue~.,data = train_data1, method= 'over', N = N_1, seed = 1)$data
  gbm.model = gbm(Revenue~., data = os_over, shrinkage = 0.01, distribution = 'bernoulli', cv.folds = 5, n.trees = 3000, verbose = F)
  best.iter = gbm.perf(gbm.model, method = "cv")
  fitControl = trainControl(method = "cv", number = 5, returnResamp = "all")
  model2 = train(as.factor(Revenue)~., data = os_over, method = "gbm", distribution = "bernoulli", trControl = fitControl, verbose = F, tuneGrid=data.frame(.n.trees=best.iter, .shrinkage=0.01, .interaction.depth=1, .n.minobsinnode=1))
  mPred = predict(model2, test_data1)
  tab <- table(mPred, as.factor(test_data1$Revenue))
  #predict
  sum_tab = colSums(tab)[1] + colSums(tab)[2]
  Accuracy[i] = sum(diag(tab))/sum_tab

  TPR[i] = tab[1]/colSums(tab)[1]
  TNR[i] = tab[4]/colSums(tab)[2]
  PPV[i] = tab[1]/rowSums(tab)[1]
  F1_neg[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  TPR[i] = tab[4]/colSums(tab)[2]
  TNR[i] = tab[1]/colSums(tab)[1]
  PPV[i] = tab[4]/rowSums(tab)[2]
  F1_pos[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  F1[i] = (F1_pos[i] + F1_neg[i])/2
}
Accuracy_gbm = mean(Accuracy)
TPR_gbm = mean(TPR)
TNR_gbm = mean(TNR)
F1_gbm = mean(F1)
```
```{r}
Accuracy_gbm
TPR_gbm 
TNR_gbm 
F1_gbm 
```

#Naive Bayes Classifier
```{r}
library(e1071)
Accuracy = rep(0, 100)
TPR = rep(0, 100)
TNR = rep(0, 100)
PPV = rep(0, 100)
F1_pos = rep(0, 100)
F1_neg = rep(0, 100)
F1 = rep(0, 100)
for (i in 1:100){
  set.seed(i)
  train=sample(1:nrow(os),ceil(0.7*nrow(os)))
  train_data = os[train,]
  test=-train
  test_data = os[test,]
  train_numerical <- train_data[,1:10] 
  train_categorical <- train_data[,11:75]
  test_numerical <- test_data[,1:10] 
  test_categorical = test_data[,11:75]
  train_scaled = scale(train_numerical)
  test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
  train_data1 <- cbind(train_scaled, train_categorical)
  test_data1 <- cbind(test_scaled, test_categorical)
  
  N_1 = 2*length( which(train_data1$Revenue == 0))
  os_under <- ovun.sample(Revenue~.,data = train_data1, method= 'over', N = N_1, seed = 1)$data
  NBC <- naiveBayes(Revenue~., data = os_under)
  test_data_predicted = predict(NBC, test_data1)
  test_data_actual = test_data1$Revenue
  tab <- table(factor(test_data_predicted), factor(test_data_actual))
  #predict
  Accuracy[i] = sum(diag(tab))/sum(tab)

  TPR[i] = tab[1]/colSums(tab)[1]
  TNR[i] = tab[4]/colSums(tab)[2]
  PPV[i] = tab[1]/rowSums(tab)[1]
  F1_neg[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  TPR[i] = tab[4]/colSums(tab)[2]
  TNR[i] = tab[1]/colSums(tab)[1]
  PPV[i] = tab[4]/rowSums(tab)[2]
  F1_pos[i] = 2*PPV[i]*TPR[i]/(PPV[i]+TPR[i])

  F1[i] = (F1_pos[i] + F1_neg[i])/2
}
Accuracy_rf = mean(Accuracy)
TPR_rf = mean(TPR)
TNR_rf = mean(TNR)
F1_rf = mean(F1)
```

```{r}
#standardize normalization for nn
library(neuralnet)
set.seed(10)
train=sample(1:nrow(os),ceil(0.7*nrow(os)))
train_data = os[train,]
test=-train
test_data = os[test,]
train_numerical <- train_data[,1:10] 
train_categorical <- train_data[,11:75]
test_numerical <- test_data[,1:10] 
test_categorical = test_data[,11:75]
train_scaled = scale(train_numerical)
test_scaled = scale(test_numerical, center=attr(train_scaled, "scaled:center"), scale=attr(train_scaled, "scaled:scale"))
train_data <- cbind(train_scaled, train_categorical)
test_data <- cbind(test_scaled, test_categorical)

N_1 = 2*length( which(train_data$Revenue == 0))
os_over <- ovun.sample(Revenue~.,data = train_data, method= 'over', N = N_1, seed = 1)$data

nn <- neuralnet(as.factor(os_over$Revenue) ~., data=os_over, hidden=c(2,1), linear.output=FALSE, threshold=0.1, algorithm = "rprop+")
nn$result.matrix
plot(nn)

#model <- mlp(train_data, train_data$Revenue, size=10, learnFunc="Rprop+",learnFuncParams=c(0.1), maxit=3000, inputsTest=test_data, targetsTest= test_data$Revenue)
#predict
pred<-predict(nn,test_data)
```
```{r}
#range normalization for nn
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
maxmindf <- as.data.frame(lapply(os, normalize))
set.seed(10)
rain=sample(1:nrow(os),ceil(0.7*nrow(os)))
train_data = maxmindf[train,]
test=-train
test_data = maxmindf[test,]
N_1 = 2*length( which(train_data$Revenue == 0))
os_over <- ovun.sample(Revenue~.,data = train_data, method= 'over', N = N_1, seed = 1)$data

nn <- neuralnet(as.factor(os_over$Revenue) ~., data=os_over, hidden=10, linear.output=FALSE, threshold=0.2, algorithm = "rprop+")
nn$result.matrix
plot(nn)

#model <- mlp(train_data, train_data$Revenue, size=10, learnFunc="Rprop+",learnFuncParams=c(0.1), maxit=3000, inputsTest=test_data, targetsTest= test_data$Revenue)
#predict
pred<-predict(nn,test_data)
```

```{r}
tab <- table(pred, test_data$Revenue)
sum_tab = colSums(tab)[1] + colSums(tab)[2] 
Accuracy = sum(diag(tab))/sum_tab
TPR= table(pred, test_data$Revenue)[1]/colSums(table(pred, test_data$Revenue))[1]
TNR = table(pred, test_data$Revenue)[4]/colSums(table(pred, test_data$Revenue))[2]
PPV = table(pred, test_data$Revenue)[1]/rowSums(table(pred, test_data$Revenue))[1]
F1_neg = 2*PPV*TPR/(PPV+TPR)

TPR = table(pred, test_data$Revenue)[4]/colSums(table(pred, test_data$Revenue))[2]
TNR = table(pred, test_data$Revenue)[1]/colSums(table(pred, test_data$Revenue))[1]
PPV = table(pred, test_data$Revenue)[4]/rowSums(table(pred, test_data$Revenue))[2]
F1_pos = 2*PPV*TPR/(PPV+TPR)

F1 = (F1_pos + F1_neg)/2
Accuracy_mlp = Accuracy
TPR_mlp = TPR
TNR_mlp= TNR
F1_mlp = F1
```

```{r}
Accuracy_mlp 
TPR_mlp 
TNR_mlp
F1_mlp
```


#Comparison
```{r}
Accuracy <- c(Accuracy_c45, Accuracy_rf, Accuracy_Linear, Accuracy_RBF, Accuracy_knn, Accuracy_gbm, Accuracy_nbc)
TPR <- c(TPR_c45, TPR_rf, TPR_Linear, TPR_RBF, TPR_knn, TPR_gbm, TPR_nbc)
TNR <- c(TNR_c45, TNR_rf, TNR_Linear, TNR_RBF, TNR_knn, TNR_gbm, TNR_nbc)
F1 = c(F1_c45, F1_rf, F1_Linear, F1_RBF, F1_knn, F1_gbm, F1_nbc)
Comp <- data.frame(Accuracy, TPR, TNR, F1)
row.names(Comp) <- c('c4.5', 'rf', 'Linear', 'RBF', 'knn', 'gbm', 'nbc')
Comp
```